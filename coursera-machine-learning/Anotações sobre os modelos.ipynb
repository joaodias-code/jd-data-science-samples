{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "João Otávio C. Dias\n",
    "\n",
    "Anotações/observações acerca dos estudos e práticas realizadas nos tipos de modelos citados abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPERVISIONADOS ##\n",
    "\n",
    "### CLASSIFICAÇÃO\n",
    "\n",
    "#### Comparações dos modelos ####\n",
    "\n",
    "1. **KNN - Vizinho mais próximo (classificação)**\n",
    "    - KNN suporta 'non-linear solutions'.\n",
    "    - KNN é 'non-parametric'.\n",
    "    - Calcular a distância no caso de dataset com multivariáveis:\n",
    "        - Dis(x1,x2) = Math.sqrt( (x1i - x2i)² )  Obs.: x1, x2, x3.. var independentes.\n",
    "    - Normalizar os dados: como o algoritmo tem que realizar os cálculos de distância, normalizar melhora este esforço de cálculo.  \n",
    "    \n",
    "    \n",
    "2. **Decision Trees**\n",
    "    - Decision tree suporta 'non-linear solutions'.\n",
    "    - Decision tree é 'non-parametric'.\n",
    "    - Trees: calcular a entropia (quanto menor melhor).. medida de aleatoridade ou incerteza. Ou seja, quanto menos incerteza num nodo (esta mais para uma medida), fácil decidir.\n",
    "    - Information gain: cálcula o nível de certeza após split dos nodos. Comparar qual split é melhor para a árvore.\n",
    "    - Comparando árvores:\n",
    "        - Decision tree: single (única árvore de decisão)\n",
    "        - Random forest: bagging (várias árvores pequenas para ver a melhor decisão)\n",
    "        - XGBoosting: boosting (pega o retorno do que não deu e verifica na próxima arvore)\n",
    "        \n",
    "\n",
    "3. **Logistic Regression**\n",
    "    - Somente 'linear solutions'\n",
    "    - Aplicações:\n",
    "        + Prever a probabilidade de uma pessoa ter ataque de coração.\n",
    "        + Prever a mortalidade de pacientes doentes.\n",
    "        + Prever a propensão de um cliente comprar um produto ou interromper uma assinatura.\n",
    "        + Prever a probabilidade de falha de um determinado processo ou produto.\n",
    "        + Prever a probabilidade de um proprietário deixar de pagar uma hipoteca.\n",
    "        \n",
    "    \n",
    "4. **Support Vector Machine**\n",
    "    - SVM suporta 'non-linear solutions'.\n",
    "    - SVM é melhor para detectar outliers do que KNN.\n",
    "    - Aplicações:\n",
    "        + Classificação de imagens\n",
    "        + Reconhecimento de escrita a mão\n",
    "        + Tarefas de mineração de textos\n",
    "        + Detecção de spam\n",
    "        + Análise de sentimentos\n",
    "        + Classificação de genes (por causa da precisão com 'high dimensional spaces')\n",
    "    - Vantagens:\n",
    "        + Precisão para 'high-dimensional spaces'\n",
    "        + Eficiente uso de memória\n",
    "    - Desvantagens:\n",
    "        + Propenso a overfitting se o número de variáveis dependentes for maior que número de amostras.\n",
    "        + Não estima probabilidade. É concebido para classificação.\n",
    "        + Não é bom para datasets grandes.\n",
    "\n",
    "#### Métricas: ####\n",
    "- _Jaccard index_: 'metrics.accuracy_score' ou 'jaccard_similarity_score' (iguais)\n",
    "- Baseados na matriz de confusão:\n",
    "\n",
    "|   | Positivo(1)  | Negativo(0) |\n",
    "|---|---        |---          |\n",
    "|   | TP        |  FN         |\n",
    "|   | FP        |  TN         |\n",
    "\n",
    "- Precision = TP / (TP + FP)\n",
    "    + Otimizar o 'precision' é o mesmo que otimizar o 'falso positivo' para ser menor.\n",
    "- Recall = TP / (TP + FN):\n",
    "    + Otimizar o 'recall' é o mesmo que otimizar o 'falso negativo' para ser menor. \n",
    "- F1-score = 2x (prc x rec) / (prc + rec)\n",
    "    + UTILIZAR: classification_report(y_test, yhat)\n",
    "    + Erro tipo 1 = FP\n",
    "    + Erro tipo 2 = FN\n",
    "- _Log loss_: usar método 'log_loss(y_test, yhat_prob)'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* * *\n",
    "## NÃO SUPERVISIONADOS ##\n",
    "\n",
    "### Clusterização\n",
    "\n",
    "#### Cluster baseado em partições ####\n",
    "\n",
    "1. **K-means**\n",
    "    - Utiliza a distância entre as amostras para modelar os clusters.\n",
    "    - Tenta minimizar a distância \"intra-cluster\" (dentro de um cluster) e maximizar a distância \"inter-cluster\" (entre os clusters). \n",
    "    - Para médio ou grande datasets (mais eficiente do que o hierarquico).\n",
    "\n",
    "#### Cluster hierárquico ####\n",
    "\n",
    "1. **Cluster Aglomerativo**\n",
    "    - Para pequenos datasets (vantagem do gráfico dendogram em relação ao cluster de partição).\n",
    "    - Aproximação \"BOTTOM -> UP\"\n",
    "    - Pares de cluster são aglorerados em direção do topo. Aglomerações: acumular. \n",
    "    - Para aglomerar, calcula-se iniciando pela menor distância entre as variáveis, e vai recalculando a matriz de proximidade e repetindo o processo.\n",
    "    - Tipos de cálculo de distância entre clusters:\n",
    "        + Single-linkage: distância minima entre os clusters.\n",
    "        + Complete-linkage: distância máxima entre os clusters (pontos mais distantes).\n",
    "        + Average-linkage: distância média entre os clusters.\n",
    "        + Centroid-linkage: distância entre os pontos-médio dos clusters.\n",
    "    \n",
    "\n",
    "2. **Cluster Divisivo**\n",
    "    - Para pequenos datasets (vantagem do gráfico dendogram em relação ao cluster de partição).\n",
    "    - Divisão \"TOP -> DOWN\"\n",
    "    - Começa com um grande cluster e quebra em peças menores. Ex.: estudo genético de raças de cães.\n",
    "\n",
    "#### Cluster baseado em densidade ####\n",
    "\n",
    "1. **DBSCAN**\n",
    "    - Algoritmo de clusterização assim como o K-means, porém com um funcionamento diferente para segmentar pontos.\n",
    "    - O DBSCAN possui dois parâmetros importantes para agrupar pontos: epsilon (Eps) e Qtd Mínima de pontos (MinPts) dentro de epsilon.\n",
    "    - Muito bom para ser utilizado para detecção de OUTLIERS, pois além de dectar os clusters, identifica aqueles que não estão dentro de um cluster, ou seja, outliers.\n",
    "    - Para isto, pode-se utilizar **PCA** para reduzir dimensões, ou seja, quando se tem muitas variáveis no dataset, o algoritmo de clusterização terá uma mapa de pontos muito dispersado e, com isto, leva-se muito tempo para calcular as distâncias entre os pontos e também pode gerar muito outlier. \n",
    "    - PCA: reduz as dimensões. Por exemplo, de 6 variáveis pode reduzir para 2. No caso se perde alguns dados, mas é aqueles com baixa correlação, ou seja, que menos influenciam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
