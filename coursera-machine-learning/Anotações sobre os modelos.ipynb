{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "João Otávio C. Dias\n",
    "\n",
    "Anotações/observações acerca dos estudos e práticas realizadas nos tipos de modelos citados abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPERVISIONADOS ##\n",
    "\n",
    "### CLASSIFICAÇÃO\n",
    "\n",
    "#### Comparações dos modelos ####\n",
    "\n",
    "1. **KNN - Vizinho mais próximo (classificação)**\n",
    "    - KNN suporta 'non-linear solutions'.\n",
    "    - KNN é 'non-parametric'.\n",
    "    - Calcular a distância no caso de dataset com multivariáveis:\n",
    "        - Dis(x1,x2) = Math.sqrt( (x1i - x2i)² )  Obs.: x1, x2, x3.. var independentes.\n",
    "    - Normalizar os dados: como o algoritmo tem que realizar os cálculos de distância, normalizar melhora este esforço de cálculo.  \n",
    "    \n",
    "    \n",
    "2. **Decision Trees**\n",
    "    - Decision tree suporta 'non-linear solutions'.\n",
    "    - Decision tree é 'non-parametric'.\n",
    "    - Trees: calcular a entropia (quanto menor melhor).. medida de aleatoridade ou incerteza. Ou seja, quanto menos incerteza num nodo (esta mais para uma medida), fácil decidir.\n",
    "    - Information gain: cálcula o nível de certeza após split dos nodos. Comparar qual split é melhor para a árvore.\n",
    "    - Comparando árvores:\n",
    "        - Decision tree: single (única árvore de decisão)\n",
    "        - Random forest: bagging (várias árvores pequenas para ver a melhor decisão)\n",
    "        - XGBoosting: boosting (pega o retorno do que não deu e verifica na próxima arvore)\n",
    "        \n",
    "\n",
    "3. **Logistic Regression**\n",
    "    - Somente 'linear solutions'\n",
    "    - Aplicações:\n",
    "        + Prever a probabilidade de uma pessoa ter ataque de coração.\n",
    "        + Prever a mortalidade de pacientes doentes.\n",
    "        + Prever a propensão de um cliente comprar um produto ou interromper uma assinatura.\n",
    "        + Prever a probabilidade de falha de um determinado processo ou produto.\n",
    "        + Prever a probabilidade de um proprietário deixar de pagar uma hipoteca.\n",
    "        \n",
    "    \n",
    "4. **Support Vector Machine**\n",
    "    - SVM suporta 'non-linear solutions'.\n",
    "    - SVM é melhor para detectar outliers do que KNN.\n",
    "    - Aplicações:\n",
    "        + Classificação de imagens\n",
    "        + Reconhecimento de escrita a mão\n",
    "        + Tarefas de mineração de textos\n",
    "        + Detecção de spam\n",
    "        + Análise de sentimentos\n",
    "        + Classificação de genes (por causa da precisão com 'high dimensional spaces')\n",
    "    - Vantagens:\n",
    "        + Precisão para 'high-dimensional spaces'\n",
    "        + Eficiente uso de memória\n",
    "    - Desvantagens:\n",
    "        + Propenso a overfitting se o número de variáveis dependentes for maior que número de amostras.\n",
    "        + Não estima probabilidade. É concebido para classificação.\n",
    "        + Não é bom para datasets grandes.\n",
    "\n",
    "#### Métricas: ####\n",
    "- _Jaccard index_: 'metrics.accuracy_score' ou 'jaccard_similarity_score' (iguais)\n",
    "- Baseados na matriz de confusão:\n",
    "\n",
    "|   | Positivo(1)  | Negativo(0) |\n",
    "|---|---        |---          |\n",
    "|   | TP        |  FN         |\n",
    "|   | FP        |  TN         |\n",
    "\n",
    "- Precision = TP / (TP + FP)\n",
    "    + Otimizar o 'precision' é o mesmo que otimizar o 'falso positivo' para ser menor.\n",
    "- Recall = TP / (TP + FN):\n",
    "    + Otimizar o 'recall' é o mesmo que otimizar o 'falso negativo' para ser menor. \n",
    "- F1-score = 2x (prc x rec) / (prc + rec)\n",
    "    + UTILIZAR: classification_report(y_test, yhat)\n",
    "    + Erro tipo 1 = FP\n",
    "    + Erro tipo 2 = FN\n",
    "- _Log loss_: usar método 'log_loss(y_test, yhat_prob)'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* * *\n",
    "## NÃO SUPERVISIONADOS ##\n",
    "\n",
    "### Clusterização\n",
    "\n",
    "#### Cluster baseado em partições ####\n",
    "\n",
    "1. **K-means**\n",
    "    - Utiliza a distância entre as amostras para modelar os clusters.\n",
    "    - Tenta minimizar a distância \"intra-cluster\" (dentro de um cluster) e maximizar a distância \"inter-cluster\" (entre os clusters). \n",
    "    - Para médio ou grande datasets (mais eficiente do que o hierarquico).\n",
    "\n",
    "#### Cluster hierárquico ####\n",
    "\n",
    "1. **Cluster Aglomerativo**\n",
    "    - Para pequenos datasets (vantagem do gráfico dendogram em relação ao cluster de partição).\n",
    "    - Aproximação \"BOTTOM -> UP\"\n",
    "    - Pares de cluster são aglorerados em direção do topo. Aglomerações: acumular. \n",
    "    - Para aglomerar, calcula-se iniciando pela menor distância entre as variáveis, e vai recalculando a matriz de proximidade e repetindo o processo.\n",
    "    - Tipos de cálculo de distância entre clusters:\n",
    "        + Single-linkage: distância minima entre os clusters.\n",
    "        + Complete-linkage: distância máxima entre os clusters (pontos mais distantes).\n",
    "        + Average-linkage: distância média entre os clusters.\n",
    "        + Centroid-linkage: distância entre os pontos-médio dos clusters.\n",
    "    \n",
    "\n",
    "2. **Cluster Divisivo**\n",
    "    - Para pequenos datasets (vantagem do gráfico dendogram em relação ao cluster de partição).\n",
    "    - Divisão \"TOP -> DOWN\"\n",
    "    - Começa com um grande cluster e quebra em peças menores. Ex.: estudo genético de raças de cães.\n",
    "\n",
    "#### Cluster baseado em densidade ####\n",
    "\n",
    "1. **DBSCAN**\n",
    "    - Algoritmo de clusterização assim como o K-means, porém com um funcionamento diferente para segmentar pontos.\n",
    "    - O DBSCAN possui dois parâmetros importantes para agrupar pontos: epsilon (Eps) e Qtd Mínima de pontos (MinPts) dentro de epsilon.\n",
    "    - Muito bom para ser utilizado para detecção de OUTLIERS, pois além de dectar os clusters, identifica aqueles que não estão dentro de um cluster, ou seja, outliers.\n",
    "    - Para isto, pode-se utilizar **PCA** para reduzir dimensões, ou seja, quando se tem muitas variáveis no dataset, o algoritmo de clusterização terá uma mapa de pontos muito dispersado e, com isto, leva-se muito tempo para calcular as distâncias entre os pontos e também pode gerar muito outlier. \n",
    "    - PCA: reduz as dimensões. Por exemplo, de 6 variáveis pode reduzir para 2. No caso se perde alguns dados, mas é aqueles com baixa correlação, ou seja, que menos influenciam.\n",
    "    \n",
    "* * *\n",
    "## SISTEMAS DE RECOMENDAÇÃO ##\n",
    "\n",
    "- Para implementar sistemas de recomendação existem duas formas:\n",
    "    + ***Memory-based:*** usa todo o conjunto de dados do usuário para gerar uma recomendação. Baseado em técnicas estátisticas para aproximar usuários ou items: \"Pearson Correlation\"; \"Cosine Similarity\"; \"Euclidian Distance\";\n",
    "    + ***Model-based:*** desenvolver modelos para tentar aprender as preferênicas do usuário. Utiliza regressão, classificação,  clusterização, etc.\n",
    "\n",
    "**TIPOS DE RECOMENDAÇÃO**\n",
    "\n",
    "1. **Content-based (Baseado em conteúdo)**\n",
    "    - Baseia-se na frase: \"Mostre-me mais do que gostei antes.\".\n",
    "    - Tenta descobrir quais são os aspectos favoritos de um usuário e fazer recomendações sobre itens que compartilham esses aspectos.\n",
    "    - Como funciona:\n",
    "        + Criar um \"vector profile\" do usuário: preencher com as avaliações do usuário (input user rating);\n",
    "        + Criar uma matriz \"one hot enconder\" com os genêros sendo as features/colunas. Exemplo: em caso de filmes, cada filme avalido pelo usuário seria uma linha e as colunas os genêros dos filmes marcados com \"1\" o genêro de cada;\n",
    "        + Então multiplica o primeiro vetor com as avaliações pela matriz das features. Ou seja, cria a matriz \"weighted feature\". Com isto gerou-se uma nova matriz: \"Weighted Matrix\";\n",
    "        + Então a partir da matriz anterior, cria-se a matriz do perfil do usuário (cada coluna é o genero com os valores somados). Só normalizar para ficar a matriz \"user profile\";\n",
    "        + Agora, basta usar esta matriz do perfil do usuário para multiplicar por qualquer matriz das opções disponíveis para o usuário. Por exemplo, em caso de filmes multiplica por uma matriz de filmes sendo que as colunas são os generos dos filmes. Após multiplicado, soma os valores por filme e ordena para achar as maiores preferências.\n",
    "    - Qual o problema desta técnica:\n",
    "        + no caso de filmes por exemplo, se você quer recomendar outr genero que ele nunca tenha assistido, não tem como fazer, pois só recomenda baseado naquilo que ele avaliou (user rating).\n",
    "        \n",
    "\n",
    "2. **Collaborative Filtering**\n",
    "    - Baseia-se na frase: \"Diga-me o que é popular entre os meus vizinhos, porque talvez eu também goste.\".\n",
    "    - Tenta encontrar grupos de usuários similares, e fornece recomendações baseadas nos gostos simlares deste grupo.\n",
    "    - TIPOS:\n",
    "        + **User-based:**\n",
    "            + baseado na similaridade dos usuários ou vizinhos.\n",
    "            + Primeiro calcula a similaridade com outros usuários (matriz de similaridade) baseados em itens em comuns que eles avaliaram (rating).\n",
    "            + Para calcular esta similaridade, usar \"Pearson Correlation\"; \"Cosine Similarity\"; \"Euclidian Distance\".\n",
    "            + Multiplica esta matriz de similaridade com as avaliações comuns dos outros usuários. Cria-se a matriz de pesos: weighted ratings.\n",
    "            + Agora tem que agregar (somar) a matriz de weighted ratings pelos itens (no caso de filmes, somar por filme). E normaliza os resultados: fazemos isso dividindo pela soma da matriz de similaridade de cada usuário.\n",
    "\n",
    "        + **Item-based:**\n",
    "            + baseado na similaridade dos itens.\n",
    "      \n",
    "     - Desafios de \"Collaborative Filtering\":\n",
    "         + Escassez de dados: quando usuários não fazem avaliações dos itens.\n",
    "         + \"Cold start\" ou partida a frio: quando é um novo usuário, não tem como recomendar.\n",
    "         + Escalabilidade: quando aumenta muito o número de usuários, o algoritmo perde performance para fazer as similaridades. Para isto, tem que usar outras técnicas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
